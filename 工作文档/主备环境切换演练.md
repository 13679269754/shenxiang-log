
# 主备环境切换演练

[toc]

## 第一阶段 备份可用性测试 - 应用连通性测试

本次仅测试备用环境数据库的可用性，测试产生的完数据丢掉。不涉及生产新产生的数据，新数据依然写入线上库-目前为航天云。航天云依然作为生产的库。


## 资源列表

| 类型 | ip | port | 用途 | 
| -- | -- | -- | -- |
| redis | 172.30.70.71 | 6100 | 后端redis |  |  
| redis | 172.30.70.72 | 6100 | 后端redis |
| redis | 172.30.70.72 | 26100 | 后端redis-sentinel |
| redis | 172.30.70.72 | 26100 | 后端redis-sentinel |
| redis | 172.30.70.73 | 26100 | 后端redis-sentinel |
| redis | 172.30.70.71 | 6000 | 算法redis |
| redis | 172.30.70.72 | 6000 | 算法redis |
| mysql | 172.30.70.41 | 3106 | 后端mysql |
| mysql | 172.30.70.42 | 3106 | 后端mysql |
| mysql | 172.30.70.43 | 3106 | 后端mysql |
| mysql | 172.30.70.44 | 3106 | 后端mysql |
| mysql | 172.30.70.45 | 3106 | 备份mysql |
| mysql | 172.30.72.21 | 3106 | 大数据取数mysql |
| mysql | 172.30.2.198 | 3106 | 大专家算法数据转换数据库 |
| mysql | 172.30.72.22 | 3106 | 大专家算法数据转换数据库 |
| proxysql | 172.30.70.31 | 6033 | proxysql-server1 |
| proxysql | 172.30.70.32 | 6033 | proxysql-server2 |
| proxysql | 172.30.70.33 | 6033 | proxysql-server3 |
| elasticsearch | 172.30.70.61 | 9200 | elasticsearch1 |
| elasticsearch | 172.30.70.61 | 9200 | elasticsearch2 |
| elasticsearch | 172.30.70.61 | 9200 | elasticsearch3 |
| influxdb | 172.30.70.73 | 8086 | influxdb |
| neo4j | 172.30.70.73 | 7474 |  neo4j | 
| orentdb | 172.30.2.179 | 2424 | orentdb1 |
| orentdb | 172.30.2.180 | 2424 | orentdb2 |
| orentdb | 172.30.2.181 | 2424 | orentdb3 |

### 操作需要考虑的点

本次验证仅验证IDC环境的可用性，不做数据同步的考虑。  
1. iptables 
2. mysql 只读状态
3. 高可用组件启停时间（确保数据集群正常后在开启高可用组件）
4. redis 哨兵模式可能会自动将原从结点，强制重新加入集群

## redis

---

1. 断开当前的主从关系;

```bash
# 去除当前主从关系（hty）
# 10.159.65.123  10.159.65.124 10.159.65.125
# 清除配置文件中的集群探测数据
sed -i.bak '/# Generated by CONFIG REWRITE/Q' /usr/local/data/redis/26000/conf/sentinel.conf
sed -i.bak '/# Generated by CONFIG REWRITE/Q' /usr/local/data/redis/26100/conf/sentinel.conf

# 哨兵重新发现集群节点 
redis-cli -p 26100 sentinel reset master-hty-6100
redis-cli -p 26000 sentinel reset master-hty-6100
# 获取当前两个redis 集群的集群状态
# 172.30.70.11 
cd ~/script
bash  redis_replication_status.sh /usr/local/bin/redis-cli 6100 bTcAsV3mTJzZGanZJaJt 172.30.70.71 172.30.70.72

bash  redis_replication_status.sh /usr/local/bin/redis-cli 6000 1hpAACExDKv5yZhN 172.30.70.71 172.30.70.72

```

```bash
# 172.30.70.7[1，2]  port 6[1,0]00
# 关闭idc-redis
redis-cli -p 6000 -a  shutdown
redis-cli -p 6100 -a  shutdown
# 确保配置文件没有节点发现信息
sed -i.bak '/# Generated by CONFIG REWRITE/Q' /usr/local/data/redis/6000/conf/redis.conf
sed -i.bak '/# Generated by CONFIG REWRITE/Q' /usr/local/data/redis/6100/conf/redis.conf

# 断开当前idc-redis主从关系 
127.0.0.1:6100> SLAVEOF NO ONE
127.0.0.1:6100> config rewrite

```

2. 建立idc redis 主从关系
   
```bash
# 172.30.70.71 6000
127.0.0.1:6000> SLAVEOF  172.30.70.72 6000
127.0.0.1:6000> config rewrite

# 172.30.70.72 6100
127.0.0.1:6100> SLAVEOF 172.30.70.71 6100
127.0.0.1:6100> config rewrite
```

3. 调整哨兵配置为当前IDC环境redis服务; -- 可以提前做

```bash
# 修改哨兵配置文件
# 算法
tee  /usr/local/data/redis/26000/conf/sentinel.conf << EOF
daemonize yes
port 26000
dir "/usr/local/data/redis/26000/sentinel"
logfile "/usr/local/data/redis/26000/log/sentinel-26000.log"
sentinel monitor master-idc-6000 172.30.70.72 6000 2
sentinel auth-pass master-idc-6000 1hpAACExDKv5yZhN
sentinel down-after-milliseconds master-idc-6000 20000
EOF

# 后端
# sentinel monitor 
tee  /usr/local/data/redis/26100/conf/sentinel.conf << EOF
sentinel announce-ip "172.30.70.71"
sentinel announce-port 26100
daemonize yes
port 26100
maxclients 10000
dir "/usr/local/data/redis/26100/sentinel"
logfile "/usr/local/data/redis/26100/log/sentinel-26100.log"
sentinel monitor master-idc-6100 172.30.70.71 6100 2
sentinel auth-pass master-idc-6100 bTcAsV3mTJzZGanZJaJt
sentinel down-after-milliseconds master-idc-6100 20000
EOF
```

4. 启动哨兵

```bash
# 172.30.70.[71,72,34]
su - redis 
./start_26100_sentinel.sh
```

5. 确认当前redis的主从状态

```bash
# 172.30.70.11 
cd ~/script
bash  redis_replication_status.sh /usr/local/bin/redis-cli 6100 bTcAsV3mTJzZGanZJaJt 172.30.70.71 172.30.70.72

bash  redis_replication_status.sh /usr/local/bin/redis-cli 6000 1hpAACExDKv5yZhN 172.30.70.71 172.30.70.72
```


## mysql 

1. 确认当前mysql 主从复制关系，以及对应gtid 点位

```bash
# 172.30.70.11 
cd ~/script
# hty的主从差距
bash  mysql_replication_status.sh /usr/local/data/mysql/bin/ dzjrep 3106 10.159.65.152 172.30.70.41
# idc集群内部的差距 
bash  mysql_replication_status.sh /usr/local/data/mysql/bin/ dzjrep 3106 172.30.70.41 172.30.70.42 172.30.70.43 172.30.70.44 172.30.70.45 172.30.72.21
```

2. 断开当前idc 主库与hty 主库的主从关系

```bash
# 172.30.70.11 
/usr/local/data/mysql/bin/mysql -h 172.30.70.41 -P 3106 -udzjrep -p -e "show slave status;"

/usr/local/data/mysql/bin/mysql -h 172.30.70.41 -P 3106 -udzjrep -p -e "stop slave;"
```

3. 启动orchestrator

```bash
ssh root@172.30.70.31 'cd /usr/local/data/orchestrator && ./start.sh'
ssh root@172.30.70.31 'ps -ef |grep orchestrator'
ssh root@172.30.70.31 "sed -i '2s/^#//' /var/spool/cron/root"

ssh root@172.30.70.32 '/usr/local/data/orchestrator/start.sh'
ssh root@172.30.70.32 'ps -ef |grep orchestrator'
ssh root@172.30.70.32 "sed -i '2s/^#//' /var/spool/cron/root"

ssh root@172.30.70.34 '/usr/local/data/orchestrator/start.sh'
ssh root@172.30.70.34 'ps -ef |grep orchestrator'
ssh root@172.30.70.34 "sed -i '2s/^#//' /var/spool/cron/root"
```

访问 orchestrator 页面查看信息，添加idc集群
https:72.30.70.31:3000  
172.30.70.41   

4. proxysql 连通性测试


5. 等待172.30.72.21 从库追上 172.30.70.41 ，将172.30.72.21 主库切换到hty 主库10.159.65.152 (前提确认全部大数据ETL组件只连接72.21) -- 可提前做（已完成） 

```bash
# 172.30.70.11 
bash  mysql_replication_status.sh /usr/local/data/mysql/bin/ dzjrep 3106 10.159.65.152 172.30.72.21
app_dzj_rwuser
process_db_user
dzj_algorithm_write
```

```sql
-- 172.30.72.21
CHANGE MASTER TO MASTER_HOST = '10.159.65.152', MASTER_USER = 'dzjrep', MASTER_PASSWORD = 'Pw6nhngeKYf6tu5a', MASTER_PORT = 3106, MASTER_AUTO_POSITION = 1, MASTER_RETRY_COUNT = 10, MASTER_HEARTBEAT_PERIOD = 1000000; 
```

## ES 

1. 全量索引同步

2. 系统运维配置修改，测试

## influxdb

1. 确认T+1同步状态是否正常

2. 系统运维配置修改，测试

## neo4j

## oritentdb

1. 确认T+1同步状态是否正常

2. 系统运维配置修改，测试